Absolutely. Here’s the clean, up-to-date Gemini integration I recommend for your Context Mirror—using Google’s current **Google Gen AI SDK** (`@google/genai`) with **structured JSON output**, **system instructions**, and model selectors for **Gemini 2.5**.

---

# 1) Install & env

```bash
npm i @google/genai
# or: pnpm add @google/genai / yarn add @google/genai
```

Set your key (server-side only):

```bash
export GEMINI_API_KEY=...   # from Google AI Studio
# optional: choose a model at deploy time
export GEMINI_MODEL=gemini-2.5-flash   # or gemini-2.5-pro
```

* `@google/genai` is Google’s latest Gen AI SDK for JS/TS and the one Google recommends; older SDKs (e.g., `@google/generative-ai`, `@google-cloud/vertexai`) aren’t where new 2.x features land. ([GitHub][1])
* Use **`gemini-2.5-flash`** for best price/latency, or **`gemini-2.5-pro`** when you want max reasoning polish. Both support **structured outputs** and **system instructions**. ([Google AI for Developers][2])

---

# 2) Server function (drop-in)

Create `services/contextMirror/gemini.ts` (Node/Edge server code only):

```ts
// services/contextMirror/gemini.ts
import { GoogleGenAI, Type } from "@google/genai";

export type ContextProfile = {
  regulatoryIntensity: "low" | "moderate" | "high";
  dataSensitivity: "low" | "moderate" | "high";
  marketClockSpeed: "slow" | "moderate" | "fast";
  integrationComplexity: "low" | "moderate" | "high";
  changeTolerance: "low" | "moderate" | "high";
  scale: "local" | "regional" | "global";
};

export type ContextMirrorPayload = {
  insight: string;      // exactly two paragraphs, 150–220 words total
  disclaimer: string;   // single-line micro-disclaimer
};

const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY! });

const MODEL =
  process.env.GEMINI_MODEL?.trim() || "gemini-2.5-flash"; // or 'gemini-2.5-pro'

const SYSTEM_INSTRUCTION = [
  "You are an executive AI strategy advisor.",
  "Write in clear, concise prose suitable for senior leaders.",
  "Base analysis ONLY on organizational context (not internal capabilities).",
  "Use probability language (often, tends to, commonly).",
  "Vendor-neutral. No numbers or benchmarks.",
  "Output EXACTLY two paragraphs (150–220 words total).",
].join(" ");

function buildUserPrompt(p: ContextProfile): string {
  return [
    "Context profile:",
    `- Regulatory intensity: ${p.regulatoryIntensity}`,
    `- Data sensitivity: ${p.dataSensitivity}`,
    `- Market clock-speed: ${p.marketClockSpeed}`,
    `- Integration complexity / legacy surface: ${p.integrationComplexity}`,
    `- Change tolerance: ${p.changeTolerance}`,
    `- Scale / geography: ${p.scale}`,
    "",
    "Write a short, board-ready Context Reflection in 2 paragraphs.",
    "Paragraph 1: what this context often enables and often constrains.",
    "Paragraph 2: what this typically implies for early AI moves (guardrails, quick wins, continuity).",
    "Avoid headings, bullets, or meta-notes. Avoid the words ‘strengths’ and ‘fragilities’.",
  ].join("\n");
}

export async function generateContextMirror(
  profile: ContextProfile
): Promise<ContextMirrorPayload> {
  const response = await ai.models.generateContent({
    model: MODEL,
    contents: buildUserPrompt(profile),
    // Use current JSON mode: responseMimeType + responseSchema
    // (SDK maps to the Gemini API 'generationConfig' fields)
    config: {
      systemInstruction: SYSTEM_INSTRUCTION,
      responseMimeType: "application/json",
      responseSchema: {
        type: Type.OBJECT,
        properties: {
          insight: {
            type: Type.STRING,
            description:
              "Two paragraphs (150–220 words total). Paragraph 1 = context enables/constraints; Paragraph 2 = implications for early AI moves.",
          },
          disclaimer: { type: Type.STRING },
        },
        required: ["insight", "disclaimer"],
        propertyOrdering: ["insight", "disclaimer"],
      },
    },
  });

  // The SDK returns JSON as a string in response.text when using JSON mode.
  // Parse and return; leave UI sanitization to your existing sanitizer.
  const raw = response.text ?? "";
  let parsed: ContextMirrorPayload | null = null;
  try {
    parsed = JSON.parse(raw);
  } catch {
    // If malformed, fail fast so caller can fall back to your rule-based copy.
  }
  if (!parsed?.insight || !parsed?.disclaimer) {
    throw new Error("Gemini returned invalid JSON payload for Context Mirror");
  }
  return parsed;
}
```

Why this shape:

* Uses **system instructions** to keep “persona + constraints” invisible to end users. ([Google AI for Developers][3])
* Forces **structured JSON** with `responseMimeType` + `responseSchema` so you always get `{ insight, disclaimer }`. ([Google AI for Developers][4])
* Targets current **Gemini 2.5** model names; switch via `GEMINI_MODEL` without code changes. ([Google AI for Developers][2])

> If you later want streaming for UX, stream to a hidden buffer and only render after the JSON is complete; mixing chunked JSON with UI is messy. The SDK supports streaming via `generateContentStream`. ([GitHub][1])

---

# 3) Route handler (Next.js example)

```ts
// app/api/context-mirror/route.ts
import { NextRequest, NextResponse } from "next/server";
import { generateContextMirror } from "@/services/contextMirror/gemini";

export async function POST(req: NextRequest) {
  const profile = (await req.json()) as Parameters<typeof generateContextMirror>[0];

  try {
    const payload = await generateContextMirror(profile);
    return NextResponse.json(payload);
  } catch (e) {
    // Your existing deterministic fallback (template A/B/C) can run here
    return NextResponse.json(
      {
        insight:
          "Your context suggests clear opportunities alongside constraints. We’re refreshing this brief to state those implications in narrative form.",
        disclaimer: "Educational reflection based on your context; not a compliance determination.",
        _fallback: true,
      },
      { status: 200 }
    );
  }
}
```

---

# 4) Vertex AI option (if you deploy on GCP)

You can flip this same code to Vertex AI by environment variables—no code rewrite:

```bash
export GOOGLE_GENAI_USE_VERTEXAI=true
export GOOGLE_CLOUD_PROJECT=your-project
export GOOGLE_CLOUD_LOCATION=us-central1
```

The SDK automatically talks to Vertex endpoints (and you can pin `apiVersion` when needed). ([GitHub][1])

---

# 5) What this updates vs. your current build

* ✅ **Latest SDK** (`@google/genai`) instead of older/legacy packages. ([GitHub][1])
* ✅ **Current model IDs** (`gemini-2.5-flash` / `gemini-2.5-pro`) with simple toggles. ([Google AI for Developers][2])
* ✅ **Structured output** guaranteed via `responseSchema` → no more leaked guideline text, counters, or malformed paragraphs. ([Google AI for Developers][4])
* ✅ **System instruction** keeps your “rules” off the page while steering tone/format. ([Google AI for Developers][3])

---

## Quick QA checklist

* [ ] Package `@google/genai` present; no `@google/generative-ai` or `@google-cloud/vertexai`. ([GitHub][1])
* [ ] Model env var defaults to `gemini-2.5-flash`; can switch to `gemini-2.5-pro`. ([Google AI for Developers][2])
* [ ] Response passes JSON parse; fields `insight` & `disclaimer` exist; two paragraphs enforced by your sanitizer before render.
* [ ] No internal guardrails (“no vendor names…”, word counts) appear anywhere in UI.

---

**Confidence:** High on SDK choice, model names, JSON/structured-output wiring, and system-instruction usage (all taken from current Google docs); medium on your exact server/runtime (if you’re using a different framework, the service function stays the same). ([GitHub][1])

[1]: https://github.com/googleapis/js-genai "GitHub - googleapis/js-genai: TypeScript/JavaScript SDK for Gemini and Vertex AI."
[2]: https://ai.google.dev/gemini-api/docs/models "Gemini models  |  Gemini API  |  Google AI for Developers"
[3]: https://ai.google.dev/api/generate-content "Generating content  |  Gemini API  |  Google AI for Developers"
[4]: https://ai.google.dev/gemini-api/docs/structured-output "Structured output  |  Gemini API  |  Google AI for Developers"
